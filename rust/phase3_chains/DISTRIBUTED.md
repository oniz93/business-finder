# Phase 3 Chains - Distributed Architecture

This document describes the distributed architecture support added to the Phase 3 chains processor.

## Overview

The Phase 3 chains processor now supports three operating modes:

1. **Standalone Mode** (default) - Single machine processing with checkpoint/resume support
2. **Coordinator Mode** - Acts as a task server, distributing work to workers
3. **Worker Mode** - Connects to coordinator, requests and processes tasks

## Architecture

### Components

#### Protocol (`protocol.rs`)
Defines the network messages exchanged between coordinator and workers:
- `RequestTask` - Worker requests a task from coordinator
- `AssignTask` - Coordinator assigns a subreddit task to worker
- `TaskComplete` - Worker reports task completion status
- `Heartbeat` - Keep-alive message from worker
- `NoTasksAvailable` - All work is done

Messages are serialized using bincode for efficient binary communication.

#### Discovery (`discovery.rs`)
mDNS service announcement and discovery:
- Coordinator announces itself on LAN as `_phase3._tcp.local.`
- Workers can auto-discover coordinator or use manual address
- Based on the `mdns-sd` crate

#### Processing (`processing.rs`)
Core subreddit processing logic extracted from original code:
- Processes a single subreddit directory
- Uses DuckDB for in-memory message chain validation
- Processes ideas in chunks for memory efficiency
- Writes validated chains to parquet files

#### Coordinator (`coordinator.rs`)
Task distribution server:
- Maintains task queue from scanned subreddits
- Tracks task states (Pending, Assigned, Completed, Failed)
- Monitors worker heartbeats (60-second timeout)
- Reassigns tasks from stale workers
- Provides status/progress information every 30 seconds

#### Worker (`worker.rs`)
Task processing client with smart caching:
- Connects to coordinator (via mDNS or manual address)
- Requests tasks and processes them
- **Smart Caching**: 
  - Multi-chunk subreddits copied to local cache before processing
  - Single-chunk subreddits processed directly on Samba
  - Results copied back to Samba and local cache cleaned up
- Sends periodic heartbeats to coordinator
- Reports task completion status

#### Standalone (`standalone.rs`)
Original single-machine logic preserved:
- Checkpoint/resume functionality
- Cached subreddit list
- Progress bar with ETA
- Single subreddit mode support

## Usage

### Standalone Mode (Backward Compatible)

Process all subreddits on a single machine:
```bash
# Use default paths
./phase3_chains

# Custom paths
./phase3_chains --data-dir /path/to/processed --output-dir /path/to/chains

# Process single subreddit
./phase3_chains --subreddit AskReddit
```

### Coordinator Mode

Run on the machine with the data:
```bash
./phase3_chains --mode coordinator \
  --data-dir /Volumes/2TBSSD/reddit/processed \
  --output-dir /Volumes/2TBSSD/reddit/chains \
  --port 5000
```

The coordinator will:
1. Scan for all subreddit directories
2. Create a task queue
3. Listen for worker connections on port 5000
4. Announce itself via mDNS as `_phase3._tcp.local.`
5. Distribute tasks to connected workers
6. Monitor worker heartbeats
7. Reassign tasks from disconnected workers
8. Print status updates every 30 seconds

### Worker Mode

Run on client machines with access to data via Samba:

#### With mDNS Auto-Discovery
```bash
./phase3_chains --mode worker \
  --data-dir /mnt/reddit/processed \
  --output-dir /mnt/reddit/chains \
  --local-cache-dir /tmp/phase3_cache
```

#### With Manual Coordinator Address
```bash
./phase3_chains --mode worker \
  --coordinator-addr 192.168.1.100:5000 \
  --data-dir /mnt/reddit/processed \
  --output-dir /mnt/reddit/chains \
  --local-cache-dir /tmp/phase3_cache \
  --worker-id worker-1
```

The worker will:
1. Connect to coordinator (auto-discover via mDNS or use manual address)
2. Request tasks from coordinator
3. Process subreddits with smart caching:
   - **Multi-chunk subreddits** (>1 parquet file): Copy to local cache â†’ process â†’ copy results back â†’ cleanup
   - **Single-chunk subreddits** (â‰¤1 parquet file): Process directly on Samba
4. Send heartbeats every 30 seconds
5. Report completion status
6. Request next task

## Smart Caching Strategy

Workers implement intelligent caching to optimize performance:

### When to Cache Locally
- **Condition**: Subreddit has more than 1 parquet file (chunk)
- **Reason**: Large subreddits benefit from local processing speed
- **Process**:
  1. Copy all input parquet files to `/tmp/phase3_cache/input_{uuid}/`
  2. Process using local files
  3. Write output to `/tmp/phase3_cache/output_{uuid}/`
  4. Copy output files to Samba mount
  5. Delete local cache directories

### When to Process Directly
- **Condition**: Subreddit has 1 or fewer parquet files
- **Reason**: Small subreddits don't justify copy overhead
- **Process**: Read from Samba, write to Samba directly

### Benefits
- **Performance**: Fast local processing for large subreddits
- **Network**: Reduced network load during processing
- **Space**: Temporary files auto-cleaned after each task
- **Safety**: Only workers delete local cache, never the coordinator/master

## Network Protocol

### TCP Communication
- Binary protocol using bincode serialization
- Length-prefixed messages (4-byte BE header + message data)
- Maximum message size: 10MB
- Async I/O using Tokio

### Message Flow

1. **Task Request**:
   ```
   Worker â†’ Coordinator: RequestTask { worker_id }
   Coordinator â†’ Worker: AssignTask { task_id, relative_path }
   ```

2. **Task Completion**:
   ```
   Worker â†’ Coordinator: TaskComplete { task_id, status }
   (No response - worker requests next task)
   ```

3. **Heartbeat**:
   ```
   Worker â†’ Coordinator: Heartbeat { worker_id }
   (No response - fire and forget)
   ```

4. **No Tasks**:
   ```
   Worker â†’ Coordinator: RequestTask { worker_id }
   Coordinator â†’ Worker: NoTasksAvailable
   (Worker exits)
   ```

### Error Handling
- **Worker disconnection**: Coordinator detects via heartbeat timeout
- **Task failure**: Worker reports failure, coordinator re-queues task
- **Network errors**: Workers log error and exit, tasks are reassigned

## Configuration Options

| Option | Description | Required | Default |
|--------|-------------|----------|---------|
| `--mode` | Operating mode | No | `standalone` |
| `--data-dir` | Input data directory | No | `/Volumes/2TBSSD/reddit/processed` |
| `--output-dir` | Output directory | No | `/Volumes/2TBSSD/reddit/chains` |
| `--subreddit` | Single subreddit mode | No | None (process all) |
| `--coordinator-addr` | Manual coordinator address | No | Auto-discover via mDNS |
| `--local-cache-dir` | Worker temp directory | No | None (no caching) |
| `--worker-id` | Worker identifier | No | Auto-generated UUID |
| `--port` | Coordinator listen port | No | `5000` |

## Example Deployment

### Scenario
- 1 coordinator (Mac with external SSD)
- 3 workers (Linux machines with Samba mounts)
- 15,432 subreddits to process

### Coordinator (Mac)
```bash
./phase3_chains --mode coordinator \
  --data-dir /Volumes/2TBSSD/reddit/processed \
  --output-dir /Volumes/2TBSSD/reddit/chains
```

### Workers (Linux machines)
```bash
# Worker 1
./phase3_chains --mode worker \
  --data-dir /mnt/reddit/processed \
  --output-dir /mnt/reddit/chains \
  --local-cache-dir /tmp/phase3_worker1 \
  --worker-id worker-1

# Worker 2
./phase3_chains --mode worker \
  --data-dir /mnt/reddit/processed \
  --output-dir /mnt/reddit/chains \
  --local-cache-dir /tmp/phase3_worker2 \
  --worker-id worker-2

# Worker 3
./phase3_chains --mode worker \
  --data-dir /mnt/reddit/processed \
  --output-dir /mnt/reddit/chains \
  --local-cache-dir /tmp/phase3_worker3 \
  --worker-id worker-3
```

### Result
- Tasks distributed evenly across 3 workers
- ~3x speedup compared to standalone mode
- Automatic load balancing and failover
- Progress visible on coordinator console

## Monitoring

### Coordinator Status
The coordinator prints status every 30 seconds:
```
ðŸ“Š Tasks: 5000 pending, 3 assigned, 10432 completed, 0 failed. Workers: 3
```

### Worker Logs
Workers log all task processing:
```
ðŸ“¦ Received task abc-123: 01/AskReddit
  Subreddit has 5 parquet files
  Using local cache (multi-chunk processing)
  Copying input data to local cache...
  Processing from local cache...
  Copying results back to Samba...
  Cleaning up local cache...
  âœ“ Task abc-123 completed successfully
```

## Troubleshooting

### Workers Can't Find Coordinator
- Verify mDNS is working on the network
- Use `--coordinator-addr` with explicit IP:port
- Check firewall rules (port 5000 TCP)

### Slow Performance on Workers
- Verify Samba mount is working correctly
- Use `--local-cache-dir` on fast local storage (SSD)
- Check network bandwidth between worker and coordinator

### Tasks Stuck in "Assigned" State
- Worker likely crashed or disconnected
- Coordinator will reassign after 60-second timeout
- Check worker logs for errors

### Out of Disk Space on Worker
- Local cache fills up if not enough space
- Increase `--local-cache-dir` partition size
- Or process directly on Samba (remove `--local-cache-dir`)

## Technical Details

### Dependencies
- `tokio` - Async runtime for network I/O
- `mdns-sd` - mDNS service discovery
- `bincode` - Binary message serialization
- `uuid` - Task and worker identifiers
- `gethostname` - Hostname resolution for mDNS
- `duckdb` - In-memory database for chain validation
- `polars` - Parquet file I/O

### Performance Characteristics
- **Network overhead**: ~1KB per task assignment
- **Memory per task**: ~100MB-1GB (depends on subreddit size)
- **Heartbeat interval**: 30 seconds
- **Worker timeout**: 60 seconds
- **Task batch size**: 1 subreddit per task

### Scalability
- Tested with 3 workers, designed for up to 100
- Linear speedup with number of workers
- Coordinator handles 1000s of tasks efficiently
- Network bandwidth is typically not a bottleneck

## Future Improvements

Potential enhancements:
- [ ] Persistent task queue (survive coordinator restarts)
- [ ] Web UI for monitoring progress
- [ ] Dynamic worker scaling based on load
- [ ] Task prioritization (process large subreddits first)
- [ ] Compression for network messages
- [ ] Authentication/encryption for worker connections
- [ ] Metrics export (Prometheus, Grafana)

## License

Same as the main project.
